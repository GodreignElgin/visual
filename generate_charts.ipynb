{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotlyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-1.34.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\godreign\\miniforge3\\envs\\llm_research\\lib\\site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\godreign\\miniforge3\\envs\\llm_research\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\godreign\\miniforge3\\envs\\llm_research\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\godreign\\miniforge3\\envs\\llm_research\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\godreign\\miniforge3\\envs\\llm_research\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.8 MB 2.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.8/14.8 MB 2.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.6/14.8 MB 2.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.8/14.8 MB 2.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.4/14.8 MB 2.3 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.9/14.8 MB 2.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.4/14.8 MB 2.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.9/14.8 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.5/14.8 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.7/14.8 MB 2.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.2/14.8 MB 2.3 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.8/14.8 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.3/14.8 MB 2.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.8/14.8 MB 2.3 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 7.1/14.8 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 7.3/14.8 MB 2.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 7.9/14.8 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 8.7/14.8 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 9.2/14.8 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 9.7/14.8 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 10.0/14.8 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.5/14.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.0/14.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.3/14.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.8/14.8 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 12.6/14.8 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.8/14.8 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.1/14.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.6/14.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.9/14.8 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.2/14.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.8/14.8 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.6 MB 1.3 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.8/11.6 MB 1.4 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.0/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.0/11.6 MB 1.2 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 1.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.1/11.6 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.1/11.6 MB 1.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.4/11.6 MB 1.3 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.6/11.6 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 1.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.7/11.6 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.9/11.6 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.0/11.6 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.8/11.6 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.0/11.6 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.3/11.6 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.1/11.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.3/11.6 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.9/11.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.1/11.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.2/11.6 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.6 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.6 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading narwhals-1.34.0-py3-none-any.whl (325 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, narwhals, plotly, pandas\n",
      "Successfully installed narwhals-1.34.0 pandas-2.2.3 plotly-6.0.1 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly pandas\n",
    "\n",
    "# üìö Imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# üìÇ Ensure output folder exists\n",
    "os.makedirs(\"visuals\", exist_ok=True)\n",
    "\n",
    "# üì• Load CSVs\n",
    "fp16 = pd.read_csv(\"data/evaluation_results_FP16.csv\")\n",
    "int8 = pd.read_csv(\"data/evaluation_results_INT8.csv\")\n",
    "int4 = pd.read_csv(\"data/evaluation_results_INT4.csv\")\n",
    "\n",
    "# üè∑Ô∏è Add precision column\n",
    "fp16[\"Precision\"] = \"FP16\"\n",
    "int8[\"Precision\"] = \"INT8\"\n",
    "int4[\"Precision\"] = \"INT4\"\n",
    "\n",
    "# üîó Combine into one DataFrame\n",
    "combined = pd.concat([fp16, int8, int4], ignore_index=True)\n",
    "\n",
    "# ‚úÖ Ensure numeric columns\n",
    "combined[\"BLEU Score\"] = pd.to_numeric(combined[\"BLEU Score\"], errors=\"coerce\")\n",
    "combined[\"Latency (ms)\"] = pd.to_numeric(combined[\"Latency (ms)\"], errors=\"coerce\")\n",
    "\n",
    "# üìå Create a unique variant ID\n",
    "combined[\"variant_id\"] = combined[\"Model\"] + \"_\" + combined[\"Precision\"]\n",
    "\n",
    "# ========== 1Ô∏è‚É£ FP16 BLEU Score Comparison ==========\n",
    "fig1 = px.bar(fp16, x=\"Model\", y=\"BLEU Score\", color=\"Model\",\n",
    "              title=\"1Ô∏è‚É£ BLEU Score Comparison Within FP16\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"Model\": \"Model\"})\n",
    "\n",
    "fig1.update_layout(xaxis_tickangle=-45)\n",
    "fig1.write_html(\"visuals/interactive_FP16_bleu.html\")\n",
    "\n",
    "# ========== 2Ô∏è‚É£ INT8 BLEU Score Comparison ==========\n",
    "fig2 = px.bar(int8, x=\"Model\", y=\"BLEU Score\", color=\"Model\",\n",
    "              title=\"2Ô∏è‚É£ BLEU Score Comparison Within INT8\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"Model\": \"Model\"})\n",
    "\n",
    "fig2.update_layout(xaxis_tickangle=-45)\n",
    "fig2.write_html(\"visuals/interactive_INT8_bleu.html\")\n",
    "\n",
    "# ========== 3Ô∏è‚É£ INT4 BLEU Score Comparison ==========\n",
    "fig3 = px.bar(int4, x=\"Model\", y=\"BLEU Score\", color=\"Model\",\n",
    "              title=\"3Ô∏è‚É£ BLEU Score Comparison Within INT4\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"Model\": \"Model\"})\n",
    "\n",
    "fig3.update_layout(xaxis_tickangle=-45)\n",
    "fig3.write_html(\"visuals/interactive_INT4_bleu.html\")\n",
    "\n",
    "# ========== 4Ô∏è‚É£ Average BLEU & Latency Across Precisions ==========\n",
    "avg_metrics = combined.groupby(\"Precision\")[[\"BLEU Score\", \"Latency (ms)\"]].mean().reset_index()\n",
    "avg_melted = avg_metrics.melt(id_vars=\"Precision\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "fig4 = px.bar(avg_melted, x=\"Metric\", y=\"Value\", color=\"Precision\", barmode=\"group\",\n",
    "              title=\"4Ô∏è‚É£ Average BLEU and Latency Across Precisions\",\n",
    "              labels={\"Value\": \"Average Value\", \"Metric\": \"Metric\"})\n",
    "\n",
    "fig4.write_html(\"visuals/interactive_avg_metrics.html\")\n",
    "\n",
    "# ========== 5Ô∏è‚É£ BLEU Comparison Across All Model Variants ==========\n",
    "fig5 = px.bar(combined, x=\"variant_id\", y=\"BLEU Score\", color=\"Precision\",\n",
    "              title=\"5Ô∏è‚É£ BLEU Score Comparison Across All Model Variants\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"variant_id\": \"Model + Precision\"})\n",
    "\n",
    "fig5.update_layout(xaxis_tickangle=-90)\n",
    "fig5.write_html(\"visuals/interactive_all_variants_bleu.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these new visualizations to your existing notebook\n",
    "\n",
    "# ========== 6Ô∏è‚É£ Performance vs. Efficiency Scatter Plot ==========\n",
    "# This shows the trade-off between BLEU score and latency\n",
    "fig6 = px.scatter(combined, x=\"Latency (ms)\", y=\"BLEU Score\", \n",
    "                  color=\"Precision\", symbol=\"Model\", size=\"BLEU Score\",\n",
    "                  hover_data=[\"Model\", \"Precision\", \"BLEU Score\", \"Latency (ms)\"],\n",
    "                  title=\"6Ô∏è‚É£ Performance vs. Efficiency Trade-off\",\n",
    "                  labels={\"BLEU Score\": \"BLEU Score (higher is better)\", \n",
    "                          \"Latency (ms)\": \"Latency in ms (lower is better)\"})\n",
    "\n",
    "# Add a trend line\n",
    "fig6.update_traces(marker=dict(line=dict(width=1, color='DarkSlateGrey')))\n",
    "fig6.update_layout(legend_title_text='Precision')\n",
    "fig6.write_html(\"visuals/interactive_performance_efficiency.html\")\n",
    "\n",
    "# ========== 7Ô∏è‚É£ Precision Degradation Analysis ==========\n",
    "# First, create a pivot table to compare the same model across different precisions\n",
    "pivot_df = combined.pivot_table(index=\"Model\", columns=\"Precision\", \n",
    "                              values=\"BLEU Score\", aggfunc=\"first\").reset_index()\n",
    "\n",
    "# Calculate degradation percentages\n",
    "for precision in [\"INT8\", \"INT4\"]:\n",
    "    pivot_df[f\"{precision} vs FP16 (%)\"] = ((pivot_df[precision] - pivot_df[\"FP16\"]) / pivot_df[\"FP16\"] * 100).round(2)\n",
    "\n",
    "# Melt the dataframe for visualization\n",
    "degradation_df = pivot_df.melt(id_vars=\"Model\", \n",
    "                              value_vars=[\"INT8 vs FP16 (%)\", \"INT4 vs FP16 (%)\"],\n",
    "                              var_name=\"Comparison\", value_name=\"Degradation (%)\")\n",
    "\n",
    "fig7 = px.bar(degradation_df, x=\"Model\", y=\"Degradation (%)\", color=\"Comparison\",\n",
    "             barmode=\"group\", title=\"7Ô∏è‚É£ BLEU Score Degradation Relative to FP16\",\n",
    "             labels={\"Degradation (%)\": \"% Change from FP16 (negative = worse)\"})\n",
    "\n",
    "fig7.update_layout(xaxis_tickangle=-45)\n",
    "fig7.write_html(\"visuals/interactive_precision_degradation.html\")\n",
    "\n",
    "# ========== 8Ô∏è‚É£ Model Size vs. Performance ==========\n",
    "# If you have model size data, you can add it to your combined dataframe\n",
    "# This is a placeholder - you'll need to add the actual model size data\n",
    "# Let's assume you have a dictionary mapping model names to their sizes in MB\n",
    "model_sizes = {\n",
    "    \"MODEL_A\": 350,\n",
    "    \"MODEL_B\": 420, \n",
    "    \"MODEL_C\": 500,\n",
    "    \"MODEL_D\": 650,\n",
    "    # Add all your models here\n",
    "}\n",
    "\n",
    "# Add model size to the combined dataframe\n",
    "combined[\"Model Size (MB)\"] = combined[\"Model\"].map(model_sizes)\n",
    "\n",
    "# Create a bubble chart\n",
    "fig8 = px.scatter(combined, x=\"Model Size (MB)\", y=\"BLEU Score\", \n",
    "                 size=\"Latency (ms)\", color=\"Precision\", symbol=\"Model\",\n",
    "                 hover_data=[\"Model\", \"Precision\", \"BLEU Score\"],\n",
    "                 title=\"8Ô∏è‚É£ Model Size vs. Performance Trade-off\",\n",
    "                 labels={\"BLEU Score\": \"BLEU Score\", \"Model Size (MB)\": \"Model Size (MB)\"})\n",
    "\n",
    "fig8.update_layout(xaxis_title=\"Model Size (MB)\")\n",
    "fig8.write_html(\"visuals/interactive_size_performance.html\")\n",
    "\n",
    "# ========== 9Ô∏è‚É£ Performance Distribution Boxplots ==========\n",
    "fig9 = px.box(combined, x=\"Precision\", y=\"BLEU Score\", color=\"Precision\",\n",
    "             points=\"all\", title=\"9Ô∏è‚É£ BLEU Score Distribution by Precision\",\n",
    "             labels={\"BLEU Score\": \"BLEU Score\", \"Precision\": \"Precision\"})\n",
    "\n",
    "fig9.write_html(\"visuals/interactive_performance_distribution.html\")\n",
    "\n",
    "# ========== üîü Performance Radar Charts ==========\n",
    "# This creates a radar chart to compare multiple metrics for each precision\n",
    "# Let's assume you have multiple metrics in your data\n",
    "# If not, you could use other columns or calculate additional metrics\n",
    "\n",
    "# Create a sample dataframe with multiple metrics\n",
    "# In a real scenario, you'd use your actual metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "    \"BLEU Score\": avg_metrics[\"BLEU Score\"].tolist(),\n",
    "    \"Speed (1/Latency)\": (1000 / avg_metrics[\"Latency (ms)\"]).tolist(),\n",
    "    \"Memory Efficiency\": [1.0, 2.0, 4.0],  # Relative to FP16\n",
    "    \"Inference Throughput\": [1.0, 1.8, 3.5]  # Relative to FP16\n",
    "})\n",
    "\n",
    "# Normalize the metrics for better visualization\n",
    "for col in metrics_df.columns:\n",
    "    if col != \"Precision\":\n",
    "        max_val = metrics_df[col].max()\n",
    "        metrics_df[col] = metrics_df[col] / max_val\n",
    "\n",
    "# Create the radar chart\n",
    "metrics_melted = metrics_df.melt(id_vars=\"Precision\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "fig10 = px.line_polar(metrics_melted, r=\"Value\", theta=\"Metric\", color=\"Precision\", line_close=True,\n",
    "                     title=\"üîü Multi-metric Performance Comparison\",\n",
    "                     range_r=[0, 1])\n",
    "\n",
    "fig10.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])))\n",
    "fig10.write_html(\"visuals/interactive_radar_chart.html\")\n",
    "\n",
    "# ========== 1Ô∏è‚É£1Ô∏è‚É£ Interactive Model Selector Dashboard ==========\n",
    "# Create a combined dashboard with model selector\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Function to create a model comparison dashboard\n",
    "def create_model_comparison(model_name):\n",
    "    model_data = combined[combined[\"Model\"] == model_name]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\"BLEU Score by Precision\", \"Latency by Precision\", \n",
    "                       \"BLEU vs Latency\", \"Precision Comparison\"),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "              [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # BLEU Score by Precision\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_data[\"Precision\"], y=model_data[\"BLEU Score\"], name=\"BLEU Score\",\n",
    "              marker_color=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Latency by Precision\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_data[\"Precision\"], y=model_data[\"Latency (ms)\"], name=\"Latency\",\n",
    "              marker_color=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # BLEU vs Latency Scatter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=model_data[\"Latency (ms)\"], y=model_data[\"BLEU Score\"], mode=\"markers+text\",\n",
    "                  marker=dict(size=12, color=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "                  text=model_data[\"Precision\"], textposition=\"top center\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # If you have the precision degradation data\n",
    "    if \"MODEL_A\" in model_sizes:  # Just a check to ensure we have the previous code executed\n",
    "        model_pivot = pivot_df[pivot_df[\"Model\"] == model_name]\n",
    "        \n",
    "        if not model_pivot.empty:\n",
    "            degradation_data = {\n",
    "                \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "                \"BLEU Score\": [\n",
    "                    model_pivot[\"FP16\"].values[0],\n",
    "                    model_pivot[\"INT8\"].values[0],\n",
    "                    model_pivot[\"INT4\"].values[0]\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            degradation_df = pd.DataFrame(degradation_data)\n",
    "            degradation_df[\"Relative\"] = degradation_df[\"BLEU Score\"] / degradation_df[\"BLEU Score\"].max()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=degradation_df[\"Precision\"], y=degradation_df[\"Relative\"], \n",
    "                      name=\"Relative Score\", marker_color=['#1f77b4', '#ff7f0e', '#2ca02c'],\n",
    "                      text=[f\"{x:.2f}\" for x in degradation_df[\"BLEU Score\"]], textposition=\"auto\"),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(height=800, width=1000, \n",
    "                     title_text=f\"1Ô∏è‚É£1Ô∏è‚É£ Detailed Analysis for {model_name}\",\n",
    "                     showlegend=False)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create individual dashboard for each model\n",
    "for model in combined[\"Model\"].unique():\n",
    "    fig = create_model_comparison(model)\n",
    "    fig.write_html(f\"visuals/interactive_dashboard_{model}.html\")\n",
    "\n",
    "# Create an index for all model-specific dashboards\n",
    "models_list = combined[\"Model\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "    \"BLEU Score\": avg_metrics[\"BLEU Score\"].tolist(),\n",
    "    \"Speed (1/Latency)\": (1000 / avg_metrics[\"Latency (ms)\"]).tolist(),\n",
    "    \"Memory Efficiency\": [1.0, 2.0, 4.0],  # Customize if needed\n",
    "    \"Inference Throughput\": [1.0, 1.8, 3.5]\n",
    "})\n",
    "\n",
    "# Normalize metrics\n",
    "for col in metrics_df.columns[1:]:\n",
    "    metrics_df[col] = metrics_df[col] / metrics_df[col].max()\n",
    "\n",
    "metrics_melted = metrics_df.melt(id_vars=\"Precision\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "fig10 = px.line_polar(metrics_melted, r=\"Value\", theta=\"Metric\", color=\"Precision\", line_close=True,\n",
    "                     title=\"üîü Multi-metric Performance Comparison\",\n",
    "                     range_r=[0, 1])\n",
    "\n",
    "fig10.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])))\n",
    "fig10.write_html(\"visuals/interactive_radar_chart.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
