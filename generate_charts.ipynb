{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from plotly) (1.33.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly pandas\n",
    "\n",
    "# üìö Imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# üìÇ Ensure output folder exists\n",
    "os.makedirs(\"visuals\", exist_ok=True)\n",
    "\n",
    "# üì• Load CSVs\n",
    "fp16 = pd.read_csv(\"data/evaluation_results_FP16.csv\")\n",
    "int8 = pd.read_csv(\"data/evaluation_results_INT8.csv\")\n",
    "int4 = pd.read_csv(\"data/evaluation_results_INT4.csv\")\n",
    "\n",
    "# üè∑Ô∏è Add precision column\n",
    "fp16[\"Precision\"] = \"FP16\"\n",
    "int8[\"Precision\"] = \"INT8\"\n",
    "int4[\"Precision\"] = \"INT4\"\n",
    "\n",
    "# üîó Combine into one DataFrame\n",
    "combined = pd.concat([fp16, int8, int4], ignore_index=True)\n",
    "\n",
    "# ‚úÖ Ensure numeric columns\n",
    "combined[\"BLEU Score\"] = pd.to_numeric(combined[\"BLEU Score\"], errors=\"coerce\")\n",
    "combined[\"Latency (ms)\"] = pd.to_numeric(combined[\"Latency (ms)\"], errors=\"coerce\")\n",
    "\n",
    "# üìå Create a unique variant ID\n",
    "combined[\"variant_id\"] = combined[\"Model\"] + \"_\" + combined[\"Precision\"]\n",
    "\n",
    "# ========== 1Ô∏è‚É£ FP16 BLEU Score Comparison ==========\n",
    "fig1 = px.bar(fp16, x=\"Model\", y=\"BLEU Score\", color=\"Model\",\n",
    "              title=\"1Ô∏è‚É£ BLEU Score Comparison Within FP16\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"Model\": \"Model\"})\n",
    "\n",
    "fig1.update_layout(xaxis_tickangle=-45)\n",
    "fig1.write_html(\"visuals/interactive_FP16_bleu.html\")\n",
    "\n",
    "# ========== 2Ô∏è‚É£ INT8 BLEU Score Comparison ==========\n",
    "fig2 = px.bar(int8, x=\"Model\", y=\"BLEU Score\", color=\"Model\",\n",
    "              title=\"2Ô∏è‚É£ BLEU Score Comparison Within INT8\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"Model\": \"Model\"})\n",
    "\n",
    "fig2.update_layout(xaxis_tickangle=-45)\n",
    "fig2.write_html(\"visuals/interactive_INT8_bleu.html\")\n",
    "\n",
    "# ========== 3Ô∏è‚É£ INT4 BLEU Score Comparison ==========\n",
    "fig3 = px.bar(int4, x=\"Model\", y=\"BLEU Score\", color=\"Model\",\n",
    "              title=\"3Ô∏è‚É£ BLEU Score Comparison Within INT4\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"Model\": \"Model\"})\n",
    "\n",
    "fig3.update_layout(xaxis_tickangle=-45)\n",
    "fig3.write_html(\"visuals/interactive_INT4_bleu.html\")\n",
    "\n",
    "# ========== 4Ô∏è‚É£ Average BLEU & Latency Across Precisions ==========\n",
    "avg_metrics = combined.groupby(\"Precision\")[[\"BLEU Score\", \"Latency (ms)\"]].mean().reset_index()\n",
    "avg_melted = avg_metrics.melt(id_vars=\"Precision\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "fig4 = px.bar(avg_melted, x=\"Metric\", y=\"Value\", color=\"Precision\", barmode=\"group\",\n",
    "              title=\"4Ô∏è‚É£ Average BLEU and Latency Across Precisions\",\n",
    "              labels={\"Value\": \"Average Value\", \"Metric\": \"Metric\"})\n",
    "\n",
    "fig4.write_html(\"visuals/interactive_avg_metrics.html\")\n",
    "\n",
    "# ========== 5Ô∏è‚É£ BLEU Comparison Across All Model Variants ==========\n",
    "fig5 = px.bar(combined, x=\"variant_id\", y=\"BLEU Score\", color=\"Precision\",\n",
    "              title=\"5Ô∏è‚É£ BLEU Score Comparison Across All Model Variants\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"variant_id\": \"Model + Precision\"})\n",
    "\n",
    "fig5.update_layout(xaxis_tickangle=-90)\n",
    "fig5.write_html(\"visuals/interactive_all_variants_bleu.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these new visualizations to your existing notebook\n",
    "\n",
    "# ========== 6Ô∏è‚É£ Performance vs. Efficiency Scatter Plot ==========\n",
    "# This shows the trade-off between BLEU score and latency\n",
    "fig6 = px.scatter(combined, x=\"Latency (ms)\", y=\"BLEU Score\", \n",
    "                  color=\"Precision\", symbol=\"Model\", size=\"BLEU Score\",\n",
    "                  hover_data=[\"Model\", \"Precision\", \"BLEU Score\", \"Latency (ms)\"],\n",
    "                  title=\"6Ô∏è‚É£ Performance vs. Efficiency Trade-off\",\n",
    "                  labels={\"BLEU Score\": \"BLEU Score (higher is better)\", \n",
    "                          \"Latency (ms)\": \"Latency in ms (lower is better)\"})\n",
    "\n",
    "# Add a trend line\n",
    "fig6.update_traces(marker=dict(line=dict(width=1, color='DarkSlateGrey')))\n",
    "fig6.update_layout(legend_title_text='Precision')\n",
    "fig6.write_html(\"visuals/interactive_performance_efficiency.html\")\n",
    "\n",
    "# ========== 7Ô∏è‚É£ Precision Degradation Analysis ==========\n",
    "# First, create a pivot table to compare the same model across different precisions\n",
    "pivot_df = combined.pivot_table(index=\"Model\", columns=\"Precision\", \n",
    "                              values=\"BLEU Score\", aggfunc=\"first\").reset_index()\n",
    "\n",
    "# Calculate degradation percentages\n",
    "for precision in [\"INT8\", \"INT4\"]:\n",
    "    pivot_df[f\"{precision} vs FP16 (%)\"] = ((pivot_df[precision] - pivot_df[\"FP16\"]) / pivot_df[\"FP16\"] * 100).round(2)\n",
    "\n",
    "# Melt the dataframe for visualization\n",
    "degradation_df = pivot_df.melt(id_vars=\"Model\", \n",
    "                              value_vars=[\"INT8 vs FP16 (%)\", \"INT4 vs FP16 (%)\"],\n",
    "                              var_name=\"Comparison\", value_name=\"Degradation (%)\")\n",
    "\n",
    "fig7 = px.bar(degradation_df, x=\"Model\", y=\"Degradation (%)\", color=\"Comparison\",\n",
    "             barmode=\"group\", title=\"7Ô∏è‚É£ BLEU Score Degradation Relative to FP16\",\n",
    "             labels={\"Degradation (%)\": \"% Change from FP16 (negative = worse)\"})\n",
    "\n",
    "fig7.update_layout(xaxis_tickangle=-45)\n",
    "fig7.write_html(\"visuals/interactive_precision_degradation.html\")\n",
    "\n",
    "# ========== 8Ô∏è‚É£ Model Size vs. Performance ==========\n",
    "# If you have model size data, you can add it to your combined dataframe\n",
    "# This is a placeholder - you'll need to add the actual model size data\n",
    "# Let's assume you have a dictionary mapping model names to their sizes in MB\n",
    "model_sizes = {\n",
    "    \"MODEL_A\": 350,\n",
    "    \"MODEL_B\": 420, \n",
    "    \"MODEL_C\": 500,\n",
    "    \"MODEL_D\": 650,\n",
    "    # Add all your models here\n",
    "}\n",
    "\n",
    "# Add model size to the combined dataframe\n",
    "combined[\"Model Size (MB)\"] = combined[\"Model\"].map(model_sizes)\n",
    "\n",
    "# Create a bubble chart\n",
    "fig8 = px.scatter(combined, x=\"Model Size (MB)\", y=\"BLEU Score\", \n",
    "                 size=\"Latency (ms)\", color=\"Precision\", symbol=\"Model\",\n",
    "                 hover_data=[\"Model\", \"Precision\", \"BLEU Score\"],\n",
    "                 title=\"8Ô∏è‚É£ Model Size vs. Performance Trade-off\",\n",
    "                 labels={\"BLEU Score\": \"BLEU Score\", \"Model Size (MB)\": \"Model Size (MB)\"})\n",
    "\n",
    "fig8.update_layout(xaxis_title=\"Model Size (MB)\")\n",
    "fig8.write_html(\"visuals/interactive_size_performance.html\")\n",
    "\n",
    "# ========== 9Ô∏è‚É£ Performance Distribution Boxplots ==========\n",
    "fig9 = px.box(combined, x=\"Precision\", y=\"BLEU Score\", color=\"Precision\",\n",
    "             points=\"all\", title=\"9Ô∏è‚É£ BLEU Score Distribution by Precision\",\n",
    "             labels={\"BLEU Score\": \"BLEU Score\", \"Precision\": \"Precision\"})\n",
    "\n",
    "fig9.write_html(\"visuals/interactive_performance_distribution.html\")\n",
    "\n",
    "# ========== üîü Performance Radar Charts ==========\n",
    "# This creates a radar chart to compare multiple metrics for each precision\n",
    "# Let's assume you have multiple metrics in your data\n",
    "# If not, you could use other columns or calculate additional metrics\n",
    "\n",
    "# Create a sample dataframe with multiple metrics\n",
    "# In a real scenario, you'd use your actual metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "    \"BLEU Score\": avg_metrics[\"BLEU Score\"].tolist(),\n",
    "    \"Speed (1/Latency)\": (1000 / avg_metrics[\"Latency (ms)\"]).tolist(),\n",
    "    \"Memory Efficiency\": [1.0, 2.0, 4.0],  # Relative to FP16\n",
    "    \"Inference Throughput\": [1.0, 1.8, 3.5]  # Relative to FP16\n",
    "})\n",
    "\n",
    "# Normalize the metrics for better visualization\n",
    "for col in metrics_df.columns:\n",
    "    if col != \"Precision\":\n",
    "        max_val = metrics_df[col].max()\n",
    "        metrics_df[col] = metrics_df[col] / max_val\n",
    "\n",
    "# Create the radar chart\n",
    "metrics_melted = metrics_df.melt(id_vars=\"Precision\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "fig10 = px.line_polar(metrics_melted, r=\"Value\", theta=\"Metric\", color=\"Precision\", line_close=True,\n",
    "                     title=\"üîü Multi-metric Performance Comparison\",\n",
    "                     range_r=[0, 1])\n",
    "\n",
    "fig10.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])))\n",
    "fig10.write_html(\"visuals/interactive_radar_chart.html\")\n",
    "\n",
    "# ========== 1Ô∏è‚É£1Ô∏è‚É£ Interactive Model Selector Dashboard ==========\n",
    "# Create a combined dashboard with model selector\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Function to create a model comparison dashboard\n",
    "def create_model_comparison(model_name):\n",
    "    model_data = combined[combined[\"Model\"] == model_name]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\"BLEU Score by Precision\", \"Latency by Precision\", \n",
    "                       \"BLEU vs Latency\", \"Precision Comparison\"),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "              [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # BLEU Score by Precision\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_data[\"Precision\"], y=model_data[\"BLEU Score\"], name=\"BLEU Score\",\n",
    "              marker_color=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Latency by Precision\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_data[\"Precision\"], y=model_data[\"Latency (ms)\"], name=\"Latency\",\n",
    "              marker_color=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # BLEU vs Latency Scatter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=model_data[\"Latency (ms)\"], y=model_data[\"BLEU Score\"], mode=\"markers+text\",\n",
    "                  marker=dict(size=12, color=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "                  text=model_data[\"Precision\"], textposition=\"top center\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # If you have the precision degradation data\n",
    "    if \"MODEL_A\" in model_sizes:  # Just a check to ensure we have the previous code executed\n",
    "        model_pivot = pivot_df[pivot_df[\"Model\"] == model_name]\n",
    "        \n",
    "        if not model_pivot.empty:\n",
    "            degradation_data = {\n",
    "                \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "                \"BLEU Score\": [\n",
    "                    model_pivot[\"FP16\"].values[0],\n",
    "                    model_pivot[\"INT8\"].values[0],\n",
    "                    model_pivot[\"INT4\"].values[0]\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            degradation_df = pd.DataFrame(degradation_data)\n",
    "            degradation_df[\"Relative\"] = degradation_df[\"BLEU Score\"] / degradation_df[\"BLEU Score\"].max()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=degradation_df[\"Precision\"], y=degradation_df[\"Relative\"], \n",
    "                      name=\"Relative Score\", marker_color=['#1f77b4', '#ff7f0e', '#2ca02c'],\n",
    "                      text=[f\"{x:.2f}\" for x in degradation_df[\"BLEU Score\"]], textposition=\"auto\"),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(height=800, width=1000, \n",
    "                     title_text=f\"1Ô∏è‚É£1Ô∏è‚É£ Detailed Analysis for {model_name}\",\n",
    "                     showlegend=False)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create individual dashboard for each model\n",
    "for model in combined[\"Model\"].unique():\n",
    "    fig = create_model_comparison(model)\n",
    "    fig.write_html(f\"visuals/interactive_dashboard_{model}.html\")\n",
    "\n",
    "# Create an index for all model-specific dashboards\n",
    "models_list = combined[\"Model\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "    \"BLEU Score\": avg_metrics[\"BLEU Score\"].tolist(),\n",
    "    \"Speed (1/Latency)\": (1000 / avg_metrics[\"Latency (ms)\"]).tolist(),\n",
    "    \"Memory Efficiency\": [1.0, 2.0, 4.0],  # Customize if needed\n",
    "    \"Inference Throughput\": [1.0, 1.8, 3.5]\n",
    "})\n",
    "\n",
    "# Normalize metrics\n",
    "for col in metrics_df.columns[1:]:\n",
    "    metrics_df[col] = metrics_df[col] / metrics_df[col].max()\n",
    "\n",
    "metrics_melted = metrics_df.melt(id_vars=\"Precision\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "fig10 = px.line_polar(metrics_melted, r=\"Value\", theta=\"Metric\", color=\"Precision\", line_close=True,\n",
    "                     title=\"üîü Multi-metric Performance Comparison\",\n",
    "                     range_r=[0, 1])\n",
    "\n",
    "fig10.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])))\n",
    "fig10.write_html(\"visuals/interactive_radar_chart.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
